#!/usr/bin/env python3
"""
Script pour convertir les donn√©es d'entra√Ænement JSON en format spaCy binaire.
"""

import json
import spacy
from spacy.tokens import DocBin
from spacy.training import Example
import random
from pathlib import Path
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_json_data(json_path: str):
    """Charge les donn√©es d'entra√Ænement depuis un fichier JSON."""
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        logger.info(f"‚úÖ Charg√© {len(data)} exemples depuis {json_path}")
        return data
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du chargement de {json_path}: {e}")
        raise

def validate_training_data(data):
    """Valide la structure des donn√©es d'entra√Ænement."""
    valid_labels = {"nom_personne", "reference_dossier", "type_analyse", "date_prelevement", "service_demandeur"}
    
    for i, (text, annotations) in enumerate(data):
        if not isinstance(text, str) or not text.strip():
            raise ValueError(f"Exemple {i}: Le texte doit √™tre une cha√Æne non vide")
        
        if "entities" not in annotations:
            raise ValueError(f"Exemple {i}: 'entities' manquant dans les annotations")
        
        entities = annotations["entities"]
        for j, (start, end, label) in enumerate(entities):
            if not isinstance(start, int) or not isinstance(end, int):
                raise ValueError(f"Exemple {i}, entit√© {j}: start et end doivent √™tre des entiers")
            
            if start >= end:
                raise ValueError(f"Exemple {i}, entit√© {j}: start ({start}) doit √™tre < end ({end})")
            
            if end > len(text):
                raise ValueError(f"Exemple {i}, entit√© {j}: end ({end}) d√©passe la longueur du texte ({len(text)})")
            
            if label not in valid_labels:
                logger.warning(f"Exemple {i}, entit√© {j}: Label '{label}' non reconnu")
    
    logger.info("‚úÖ Validation des donn√©es r√©ussie")

def create_spacy_examples(nlp, data):
    """Convertit les donn√©es JSON en exemples spaCy."""
    examples = []

    for i, (text, annotations) in enumerate(data):
        try:
            doc = nlp.make_doc(text)
            entities = annotations["entities"]

            # Trier les entit√©s par position pour √©viter les chevauchements
            entities = sorted(entities, key=lambda x: (x[0], x[1]))

            # Filtrer les entit√©s qui se chevauchent
            filtered_entities = []
            for start, end, label in entities:
                # V√©rifier qu'il n'y a pas de chevauchement avec les entit√©s pr√©c√©dentes
                overlap = False
                for prev_start, prev_end, _ in filtered_entities:
                    if not (end <= prev_start or start >= prev_end):
                        overlap = True
                        break

                if not overlap and start < end and end <= len(text):
                    filtered_entities.append((start, end, label))

            # Cr√©er les spans d'entit√©s
            spans = []
            for start, end, label in filtered_entities:
                span = doc.char_span(start, end, label=label, alignment_mode="contract")
                if span is None:
                    # Essayer avec un mode d'alignement diff√©rent
                    span = doc.char_span(start, end, label=label, alignment_mode="expand")
                    if span is None:
                        logger.warning(f"Impossible de cr√©er un span pour '{text[start:end]}' ({start}-{end})")
                        continue
                spans.append(span)

            # Cr√©er le doc avec les entit√©s (seulement si on a des spans valides)
            if spans:
                doc.ents = spans
                example = Example.from_dict(doc, {"entities": [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]})
                examples.append(example)
            else:
                logger.warning(f"Exemple {i} ignor√©: aucun span valide")

        except Exception as e:
            logger.warning(f"Erreur lors du traitement de l'exemple {i}: {e}")
            continue

    logger.info(f"‚úÖ Cr√©√© {len(examples)} exemples spaCy valides")
    return examples

def split_data(examples, train_ratio=0.8):
    """Divise les donn√©es en ensembles d'entra√Ænement et de validation."""
    random.shuffle(examples)
    split_idx = int(len(examples) * train_ratio)
    
    train_examples = examples[:split_idx]
    dev_examples = examples[split_idx:]
    
    logger.info(f"‚úÖ Division: {len(train_examples)} entra√Ænement, {len(dev_examples)} validation")
    return train_examples, dev_examples

def save_spacy_data(examples, output_path):
    """Sauvegarde les exemples au format spaCy binaire."""
    doc_bin = DocBin()
    
    for example in examples:
        doc_bin.add(example.reference)
    
    doc_bin.to_disk(output_path)
    logger.info(f"‚úÖ Donn√©es sauvegard√©es dans {output_path}")

def augment_data(data, num_augmentations=0):
    """D√©sactive l'augmentation pour √©viter les probl√®mes de spans."""
    # D√©sactiver l'augmentation pour l'instant car elle cause des probl√®mes
    logger.info(f"‚úÖ Donn√©es conserv√©es sans augmentation: {len(data)} exemples")
    return data

def main():
    """Fonction principale de conversion des donn√©es."""
    # Chemins
    json_path = "training/data/spacy_format/train_data.json"
    train_output = "training/data/spacy_format/train_data.spacy"
    dev_output = "training/data/spacy_format/dev_data.spacy"
    
    # Cr√©er les dossiers si n√©cessaire
    Path(train_output).parent.mkdir(parents=True, exist_ok=True)
    
    # Charger le mod√®le spaCy
    try:
        nlp = spacy.load("fr_core_news_md")
        logger.info("‚úÖ Mod√®le fr_core_news_md charg√©")
    except OSError:
        logger.error("‚ùå Mod√®le fr_core_news_md non trouv√©. Installez-le avec: python -m spacy download fr_core_news_md")
        return
    
    # Charger et valider les donn√©es
    data = load_json_data(json_path)
    validate_training_data(data)
    
    # Augmenter les donn√©es
    augmented_data = augment_data(data, num_augmentations=3)
    
    # Convertir en exemples spaCy
    examples = create_spacy_examples(nlp, augmented_data)
    
    # Diviser les donn√©es
    train_examples, dev_examples = split_data(examples, train_ratio=0.8)
    
    # Sauvegarder
    save_spacy_data(train_examples, train_output)
    save_spacy_data(dev_examples, dev_output)
    
    logger.info("üéâ Conversion termin√©e avec succ√®s!")
    logger.info(f"üìä Statistiques finales:")
    logger.info(f"   - Entra√Ænement: {len(train_examples)} exemples")
    logger.info(f"   - Validation: {len(dev_examples)} exemples")

if __name__ == "__main__":
    main()
